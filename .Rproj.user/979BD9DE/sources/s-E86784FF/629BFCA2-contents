## Restore session
ls()  # empty environment
getwd()  # make sure you are in the right folder, where we left off yesterday
# if not, you can use setwd("directory_name") or use RStudio's Session > Set working directory (Files pane may be confusing)
load("day1.RData")  # now you can load yesterday's session
ls()  # our objects are there now!

## Packages - to extend R by providing new functions
installed.packages()  # or look at the Packages pane in Rstudio
# go to CRAN to see packages (cran.r-project.org), or Google them
install.packages("gplots")  # install gplots package from CRAN
install.packages("edgeR")  # install edgeR package from Bioconductor - this doesn't work! install.packages only works for CRAN!

# go to www.bioconductor.org
# click on "Install Bioconductor"
# run the commands as indicated
source("http://bioconductor.org/biocLite.R")
biocLite()

biocLite("edgeR")  # now this works because biocLite installs Bioconductor packages
biocLite("gplots")  # biocLite also installs CRAN packages!
installed.packages()

## Help system
?setwd  # get info about function
help(package="gplots")  # get info about packages - look at function heatmap.2
?heatmap.2  # requesting help for heatmap.2 doesn't work - why? Look at packages pane, there is no checkmark next to the gplots package. This means the package is not loaded. Indeed let's try to use heatmap.2
heatmap.2()  # this does not work either
library("gplots")  # let's load the package
heatmap.2()  # now the function is there, only we did not provide any info to use it. Notice the parentheses. What are they for?
heatmap.2  # without parentheses, R shows the actual code behind the function. It's open source!
# now that you know the library() function, next time that you restart R and want to use the biocLite() function, you need to load the package BiocInstaller

## Factors (for categorical variables) - difficult to work with if you don't know how to handle them

iris  # do you have this dataset loaded? If not, load it with data(iris)
str(iris)  # look, the column Species is a "Factor". What is that?
iris$Species  # looks like a character vector. Is that right?
is.character(iris$Species)  # nope. So let's convert it to character and see if there are differences
as.character(iris$Species)  # scroll up. Now content has double quotes. This is R's way to tell you the difference between Factor and Character vectors
table(iris$Species)  # it counts occurrences of LEVELS of a FACTOR (notice nomenclature). Who can come up with an idea to transform counts to proportions?
table(iris$Species) / length(iris$Species)  # Vectorized operation to get proportions!

species.char <- as.character(iris$Species)  # transform a factor into a character vector
species.char
str(species.char)  # it worked
table(species.char)  # table only works with factors, is that right? But here it works with character vectors too! Why? Because R is smart and many times try to guess what you want to do, so it will make the right conversions. But not always!

# for nominal (non-ordered categorical) variables
diabetes <- c("Type1", "Type2", "Type1", "Type1")
diabetes
diabetes <- factor(diabetes)  # factor takes a vector (usually a character vector) and transforms it to a factor vector
diabetes
levels(diabetes)  # show the levels of a factor

# optional: for ordinal (ordered categorical) variables
# status <- c("Poor", "Improved", "Excellent", "Poor")
# status <- factor(status, order=TRUE, levels=c("Poor", "Improved", "Excellent"))
# status  # see that levels have specific orders
# levels(status)

# add new level
diabetes[5] <- "Type3"  # this doesn't work! Why?
diabetes
diabetes.char <- as.character(diabetes)
diabetes.char
diabetes.char[6] <- "Type3"  # this works. The problem with factors was that elements of factors are restricted to the pre-specified levels
diabetes
diabetes <- factor(diabetes, levels=c(levels(diabetes), "Type3"))  # so let's add a new level!
diabetes  # note that we have an extra level now
diabetes[6] <- "Type3"
diabetes  # this worked

# rename level
levels(diabetes)[levels(diabetes) == "Type3"] <- "New type"  # the commands before were for adding a new level. How about chaning existing ones?
diabetes

# delete levels
diabetes <- diabetes[1:4]
diabetes  # notice anything weird? The element "New type" is gone, but the level is still there
diabetes <- droplevels(diabetes)  # drop unused levels
diabetes  # now "New type" is gone

# what factors really are
as.numeric(diabetes)  # elements are just indexes of levels

# change order of levels
diabetes <- factor(diabetes, levels=c("Type2", "Type1"))
diabetes  # elements are still the same, but the order of levels is different
as.numeric(diabetes)  # you can also tell from the indexes
diabetes <- relevel(diabetes, ref="Type1")  # often you only need to get the reference level in the first position, and any other level can get in any order
diabetes

# factors may be difficult to work with because they require to take care of both elements and levels when manipulating them. Alternatively, you can convert factors to character vectors, perform manipulations, and then convert back character vectors to factors

## Lists (ordered collection of objects)
g <- "My First List"
h <- c(25, 26, 18, 39)
j <- matrix(1:10, nrow=5)
k <- c("one", "two", "three")
my.list <- list(title=g, ages=h, j, k)

my.list[[2]]  # output is a vector
my.list[["ages"]]  # output is a vector
my.list$ages  # output is a vector
my.list[2]  # output is a list

my.list[[2]][3]  # returns 18
my.list[[c(2, 3)]]  # equivalent to previous instruction

length(my.list)
length(my.list[[4]])  # why is that different from the command above?

# optional: flatten a list
# my.vector <- unlist(my.list)

## Loading datasets from and saving datasets to text files
myexp <- read.delim("day2.txt", header=T)  # works with tab-delimited text files. Be careful, sometimes text files in Mac/Windows have incompatible carriage return characters, and R won't load them. In Excel 2011 for Mac, exporting data as "Tab-delimited Text File" works.
# read.csv is the function for CSV files
# read.table is the generic function
str(myexp)
write.table(myexp, file="day2bis.txt", quote=F, sep="\t", row.names=T, col.names=T)  # always use quote=F, while other options can be changed depending on your needs. For CSV files, the sep character should be ","

## Conditionals

x <- 2

if(x > 3) {  # curly braces indicate beginning and end of a set of instructions
	print("your number is greater than 3")  # executes this if condition is TRUE. Notice the indentation - it's good programming style
} else {  # else is not necessary if you don't have instructions for when the condition is FALSE.
	print("your number is less than 3")
}  # notice that closing the curly braces is aligned with the statement that opens it

x <- 4  # try again the if conditional now

if(x > 3) {
	y <- x * 2
} else {
	y <- x
}

y <- ifelse(x > 3, x * 2, x)  # this is a more compact version, good for one liners

## Loops

# loops are computationally inefficient in R! Try using vectorized operations whenever possible

for(i in 1:nrow(myexp)) {  # i is the iterator (counter)
	print(i)  # not necessary, but to demonstrate how it works
}
print(i)  # last value

for(i in 1:nrow(myexp)) {
	myexp$Abundance[i] * 2  # why R doesn't print anything?
}
myexp$Abundance[i] * 2  # but here it prints!

for(i in 1:nrow(myexp)) {
	print(myexp$Abundance[i] * 2)  # inside loops, you need to be explicit about printing
}

# did you really need a for loop to achieve what we did? Always ask yourself: can I write the same code using vectorized operations?

myexp$Abundance * 2  # this achieves the same result, it is faster, it is more elegant, and results are printed as a single vector

for(i in myexp$Abundance) {  # unlike many other programming languages, iterators can be more than counters, they can actually store values... iteratively!
	print(i)
}

# iterates over matrices
data(iris)
m.iris <- data.matrix(iris[, 1:4])
sum(m.iris)  # sum all the matrix! Not by columns or rows!
summary(m.iris)  # by contrast, this works on columns. Different functions, different behaviours!
apply(m.iris, 1, sum)  # get sums by row (1 = rows)
apply(m.iris, 2, sum)  # get sums by column  (2 = columns)
apply(m.iris, 2, sum/length)  # this doesn't work
apply(m.iris, 2, function(x) sum(x)/length(x))  # this works. Why? Let's break it down.
apply(m.iris, 2, function(x) sum(x))  # this is the same as apply(m.iris, 2, sum)
apply(m.iris, 2, function(x) length(x))  # this is the same as apply(m.iris, 2, length)
# good, now we got it. But length(x) is actually constant, right? So this would be easier:
apply(m.iris, 2, sum) / nrow(m.iris)  # this code is more elegant
apply(m.iris, 2, mean)  # this is even better
colMeans(m.iris)  # and this is the best. Always ask yourself: is there a better way to write it?

# iterates over lists
list.lengths <- lapply(my.list, length)
list.lengths  # notice that result is a list!
str(list.lengths)  # here we go! However, each element has length 1. So wouldn't it nice to simplify it?
simplify2array(list.lengths)  # simplifies to vectors (1 dimension), matrixes (2 dimensions) or arrays (> 2 dimensions), whatever makes most sense
sapply(my.list, length)  # this is the same as simplify2array(lapply(my.list, length))
lapply(my.list, sum)  # doesn't work! It must make sense. Lists can be heterogeneous.

# operations on all rows or columns of data frames are similar to matrices. But data frames, unlike matrixes, may contain factors that can be used to summarize numeric data. So there are special instructions to handle that.
myexp
by(myexp$Abundance, myexp[, c("Genotype", "Gene")], mean)
aggregate(Abundance ~ Genotype + Gene, data=myexp, FUN=mean)  # notice the formula-like syntax, that doesn't want double quotes for factors!

# do you know the difference between "tall" and "wide" table format? You can see that with Excel's Pivot tables
# data should ALWAYS be in TALL format. R assumes that! Indeed:
str(myexp)  # look, rows are called OBSERVATIONS and columns VARIABLES. No more than one observation per row!
# optional for intermediate level: to move between "tall" and "wide" data.frame formats, have a look at the reshape2 package, particularly the functions melt and dcast

## Custom functions

my.function <- function(x, y, z) {
	a <- x + y - z
	return(a)  # not essential, function will return the value computed last
}

# exercise: compute a mirror image of a matrix
test.matrix <- data.matrix(iris[1:4, 1:4])
colnames(test.matrix) <- 1:4
mirror.matrixes <- function(x) x[nrow(x):1, ncol(x):1]  # one-liner solution

# notice the use of a in the following example

a <- 10
a
my.function <- function(x, y, z) {
	a <- x + y - z
	print(a)  # the function now prints the result of its computation
	return(a)
}
result <- my.function(15, 10, 5)
result
a  # why? "a" cannot be assigned without a function, because it belongs to the parent namespace. So R interprets the "inner" a and "outer" a as different objects. But try not to put yourself in these situations. Just use different names.

# see this example too:
a <- 10
a
my.function <- function(x, y, z) {
	b <- a + x + y - z
	print(a)  # notice we are printing "a", not "b" which is the result of our computation
	return(b)
}
result <- my.function(10, 15, 5)  # the value of a is that of the "outer" a because there is no attempt to assign the variable a in the function, only to read its value
result
a

# this can be even more confusing, but it makes sense if you follow what we just saw above!
a <- 10
a
my.function <- function(x, y, z) {
	print(a)  # first time
	a <- a + x + y - z
	print(a)  # second time
	return(a)
}
result <- my.function(10, 15, 5)
result
a

# a word on parameters too
v <- 10
my.function <- function(x, y, z) {
	a <- x + y - z
	return(a)
}
result <- my.function(v, 5, 6)  # we can pass variables to functions
result

# what if we had written the function like this:
v <- 10
my.function <- function(x, y, z) {
	x <- x + y - z
}
result <- my.function(10, 5, 6)
result  # it works like the previous function
result <- my.function(v, 5, 6)
result  # yes, expected
v  # understand why? the v we pass to the function is just a copy, not the original. Functions should never change the value of parameters in good programming style!

# notice that I can call my custom functions with named parameters too
result <- my.function(x=10, y=5, z=6)
result
result <- my.function(x=10, z=6, y=5)  # this allows me to swap the order of parameters
result  # same result
my.function(x=10, z=6, y=5)  # notice that function calls do not explicity prints results. Same goes for loops.
(my.function(x=10, z=6, y=5))  # but you can force printing with round brackets

## Correlation
x <- c(1, 2, 3, 4, 5, 6)
y <- c(2, 4, 6, 8, 10, 12)
cor(x, y)  # perfect correlation! This is Pearson's correlation.
y <- c(2, 6, 7, 12, 12, 13)
cor(x, y)
cor(x, y) ^2  # this is the R2. Method MUST be Pearson (default)!
cor(x, y, method="spearman")  # non-parametric correlation

## t-test
t.test(x, y)  # by default, two-sided and unequal variance
t.test(x, y, alternative="less")
t.test(x, y, alternative="greater")
t.test(x, y, var.equal=T)  # standard t-test (equal variance)
t.test(x, y, paired=T)  # paired t-test
result <- t.test(x, y)
result
str(result)  # interesting! It's a list
result$p.value  # this way I can extract the p-value and make computations with it
result[[3]]  # same thing

## Multiple comparison correction
p.values <- c(0.052, 0.011, 0.98, 0.54, 1, 0.002, 0.12)  # vector of p-values, for instance from multiple t-tests
p.adjust(p.values, method="bonferroni")  # Bonferroni correction, common in GWAS - stringent!
p.adjust(p.values, method="fdr")  # false discovery rate, common in genomics - permissive!
p.adjust(p.values, method="BH")  # same as FDR (Benjamini-Hochberg)
# see more at http://www.graphpad.com/guides/prism/6/statistics/index.htm?beware_of_multiple_comparisons.htm
# see more at http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2907892/pdf/nihms218402.pdf

save.image("day2.RData")
savehistory()
q()